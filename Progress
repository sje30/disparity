Progress on Disparity Program
-----------------------------


* Sat Nov 11 1995

Got the net structure set up and the parameter file in place.  

* Sun Nov 12 1995
Initial weights created at random and can be written out
see file dispwts.c

* Input vectors kind of handled - see dispInputs.c
function getInputVector() gets the nextInputVector.

* Activation of cells calculated calcActivation() in dispnet.c
works in a slowsum or fast method (use slow to check.)

* Mon Nov 13 1995

** preCellInfo added - can now determine which output cells are
connected to by an input cell. This is the reverse of cellInfo.

* Thu Nov 16 1995 
Sorted the learniing algorithm on paper. Now need to implement it.
This will involve several functions:
	readInData:
	read in the data for each eye and also the shift arrays 

	define and create the virtual output array.
	? what size is it?  2d or 1d?

	redefine the activations array - it really should be
	activations for total input to a cell, and then output from
	the cell.

	Write function to run over all of the input sequence, creating
	and storing the activations.  We will actually need the
	activation levels of all of the cells for each input presented
	I think.

	when writing derivative of tanh function, must consider using
	the better derivative 1.0 / cosh^2 x rather than (1- tanh^2 x)

	Check the convolution procedures.

	Where will the delta's be stored?
	Propagate the delta's back along the cell layers.

	Jim: example code that calls the cong_grad.
	why use log2 in the half life calcs, rather than ln (natural
	log) , and if so, where is log2() defined on system?

	What does  batch vs on line correspond to in our learning
	system.?

	correlation between disparity and output from cells.


* writing convolution programs - including wrap around.

Things to do
------------

--

Tue Nov 21 1995
About to include the CG code and the function checker.


Fri Dec  8 1995
---------------

Now have a simple version working POST Nips.  Correlation code
included to see how well the program was doing.
Two bugs in CG code meant that things werent working:

	1.  #define FALSE was -1.0 instead of 0!
	2.  #define JMAX was identical to JMIN, and so epsilon was
	    always decreasing, regardless of the value of pi.  This
	    is now fixed.

Sat Dec  9 1995
---------------
Snapshot sat9dec created.

About to work on dispinputs.c so that it can read in 2d images as well
as 1d images. Will be changing inputSkip to inputSkipX and creating an
extra param inputSkipY with default value 0.  Older parameter files
will need to be careful, as inputSkip will no longer be recognised.

Corrn.dat file created to keep track of the correlations.

sin 5 x 1000 period 500 and period 200 ok.


What about the gauss?

trying the network. a 200 x 200 output grid is too big for the net to
run - we get memory allocation probs.

gauss600.prm  - 20 x 20  (t1)

gauss600a.prm - 60 x 60

gauss600b.prm - 40 x 40

gauss600c.prm - 100 x 100. ok, but uhalf of 2 and vhalf of 200 are too
small, as seen by looking at umask and vmask. need bigger masks I feel.


- to do: run on egg box 500 x 500 would be nice idea.
 

Sun Dec 10 1995
---------------

Daily snapshot - sun10dec created.

100x100 (gauss600c) ok , but taking a very long time.  20 hours to do
500 iterations.  This could be speeded up by analysing conj grad and
only evaluating the derivative if the weights are the same as last
time.
But dont worry about this yet.

Need to write code so that the function can we can read in two images
and just get the output without needing to do any training.

This is done - see the params initWts, doLearning


7-45pm.  Sin 500 x 500 image 2 data is corrupted:

/tmp/JIMS/sin_500_by_500_period500 3 wc *
wc *
     501  250005 2365839 image1.txt
     118   58418  552960 image2.txt
     501  250005 2375527 shifts.txt
    1120  558428 5294326 total

Example: tested net on gauss600 and then checked performacnce on
eggbox data.
(testnet ~/disparity/gauss600.40x40.prm to train)
(testnet ~/disparity/checkegg.prm to test)

results in /rsunq/vision/stephene/dispruns/testthench


